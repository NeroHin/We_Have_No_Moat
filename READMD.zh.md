以下文字是一份最近被泄露的文件，由一位匿名人士在公共Discord服务器上分享，并已授权重新发布。它来自谷歌内部的一名研究员。我们已经核实了其真实性。唯一的修改是格式调整和删除了指向内部网页的链接。该文件仅代表谷歌员工的观点，而非整个公司。我们并不同意下面所写的观点，我们咨询的其他研究人员也不同意，但我们将为订阅者在另一篇文章中发表我们的意见。我们只是用来分享这个引发了很多有趣观点的文件的工具。

## 我们没有护城河

### OpenAI也没有

我们一直在关注OpenAI的动态。谁会达到下一个里程碑？下一步会是什么？

但令人不安的事实是，我们并没有摆好位置赢得这场军备竞赛，OpenAI也是如此。在我们争论的时候，第三方势力已经在悄悄地抢占我们的市场份额。

我当然是在谈论开源。简单地说，他们已经把我们甩在身后。我们认为的“重大开放性问题”已经得到解决，并被广泛应用。仅举几个例子：

 - 手机上的LLMs：人们可以在Pixel 6手机上以每秒5个令牌的速度运行基础模型。
 - 可扩展的个人AI：你可以在一个晚上在你的笔记本电脑上微调一个个性化的AI。
 - 负责任的发布：这个问题并没有被“解决”，而是被“消除”。有整个网站充满了没有任何限制的艺术模型，而文本也即将迎头赶上。
 - 多模态：目前多模态ScienceQA领域的最新技术成果仅用了一个小时就训练出来了。

虽然我们的模型在质量上仍然占有轻微优势，但差距正在以惊人的速度缩小。开源模型更快、更具可定制性、更注重隐私，性价比更高。他们用100美元和130亿参数做到了我们在1000万美元和5400亿参数下努力实现的事情。而且他们只花了几周，而不是几个月。这对我们有深远的影响：

 - 我們沒有秘密武器。我們最好的希望是學習和與谷歌以外的其他人合作。我們應該優先考慮支持第三方整合。
 - 當免費、無限制的替代品在質量上相當時，人們不會為受限制的模型付費。我們應該考慮我們的附加價值究竟在哪裡。
 - 巨大的模型正在拖慢我們的步伐。從長遠來看，最好的模型是那些可以快速迭代的模型。現在我們知道在<20B參數範疇內有什麼可能，我們應該將小型變體視為不只是一個附帶的想法。

![https://lmsys.org/blog/2023-03-30-vicuna/](https://i.imgur.com/SsMbPhF.png)

## 發生了什麼事

三月初，開源社區得到了他們第一個真正有能力的基礎模型，因為Meta的LLaMA被泄露給了公眾。它沒有指令或對話調整，也沒有RLHF。然而，社區立刻意識到了他們得到的東西的重要性。

隨後是一波巨大的創新浪潮，每隔幾天就有重大的發展（詳見完整的時間表）。短短一個月後，我們看到了帶有指令調整、量化、質量改進、人類評估、多模態、RLHF等等功能的變體，其中很多是相互構建的。

最重要的是，他們在很大程度上解決了擴展問題，使得任何人都可以嘗試。許多新想法來自普通人。訓練和實驗的門檻已經從一個大型研究機構的全部產出降低到一個人、一個晚上和一台性能強大的筆記本電腦。

## 為什麼我們本可以預見

在很多方面，這對任何人來說都不應該是一個驚喜。開源LLM的當前復興緊跟在圖像生成復興之後。社區並未忽略這些相似之處，許多人將這稱為LLM的“穩定擴散時刻”。

在這兩種情況下，通過稱為低秩適應（LoRA）的更便宜的微調機制，以及規模上的重大突破（用於圖像合成的潛在擴散，用於LLM的Chinchilla），都使得低成本的公共參與成為可能。在這兩種情況下，獲得足夠高質量的模型引發了來自世界各地個人和機構的一系列想法和迭代。在這兩種情況下，這些迅速超越了大型參與者。

在圖像生成領域，這些貢獻起到了關鍵作用，使穩定擴散與Dall-E走上了不同的道路。擁有開放模型導致了產品整合、市場、用戶界面和創新，這些都是Dall-E未曾實現的。

效果是顯而易見的：在文化影響方面迅速壓倒了OpenAI的解決方案，後者變得越來越無關緊要。對於LLM是否會發生同樣的事情，尚有待觀察，但結構層面的相似性是相同的。

## 我們錯過了什麼
驅動開源近期成功的創新直接解決了我們仍在努力解決的問題。更加關注他們的工作可以幫助我們避免重複發明輪子。

LoRA是一個非常強大的技術，我們可能應該更加關注
LoRA通過將模型更新表示為低秩分解來工作，這可以將更新矩陣的大小降低多達幾千倍。這使得模型微調的成本和時間大大降低。在幾個小時內在消費者硬件上個性化語言模型是一個重要的突破，特別是對於涉及在近實時內納入新的和多樣化知識的願景。這項技術的存在在谷歌內部被低估了，儘管它直接影響到我們一些最具野心的項目。

## 重新訓練模型是困難的道路

使LoRA如此有效的部分原因是它（像其他形式的微調一樣）是可堆疊的。像指令調整這樣的改進可以應用，然後在其他貢獻者添加對話、推理或工具使用時獲得優勢。雖然單個微調是低秩的，但它們的總和不一定是，允許隨著時間的推移，對模型進行全秩更新。

這意味著隨著新的更好的數據集和任務變得可用，模型可以以低成本保持最新狀態，而無需支付完整運行的成本。

相比之下，從頭開始訓練巨型模型不僅丟掉了預訓練，還丟掉了已經在頂部進行的迭代改進。在開源世界，這些改進很快就會佔據主導地位，使得完全重新訓練變得極為昂貴。

我們應該仔細考慮每個新的應用或想法是否真的需要一個全新的模型。如果我們確實有重大的架構改進，使我們無法直接重用模型權重，那麼我們應該投資於更具侵略性的蒸餾形式，以保留上一代模型盡可能多的功能。


## 長期來看，如果我們可以在小型模型上更快迭代，大型模型並不一定更具能力

對於最受歡迎的模型大小，LoRA更新非常便宜（〜100美元）。這意味著幾乎任何有想法的人都可以生成一個並分發出去。訓練時間在一天以內是常態。以這種速度，這些微調的累積效應很快就會克服起初的尺寸劣勢。事實上，從這些模型中改進的速度在工程師小時方面遠超我們對最大變種的能力，最好的模型已經與ChatGPT幾乎無法區分。專注於維護地球上一些最大的模型實際上讓我們處於劣勢。

## 數據質量比數據大小更具擴展性

許多這些項目通過在小型、精心策劃的數據集上進行訓練來節省時間。這表明數據縮放法則具有一定的靈活性。這些數據集的存在源於《數據並非如你所想》中的思路，它們迅速成為谷歌之外進行訓練的標準方法。這些數據集是使用合成方法（例如從現有模型中過濾最佳響應）和從其他項目中收集而來的，這兩者在谷歌都不占主導地位。幸運的是，這些高質量數據集是開源的，因此可以免費使用。

## 直接與開源競爭是輸不起的

這些最近的進展對我們的商業策略有直接、立即的影響。如果有免費、高質量的替代品而沒有使用限制，誰會願意付錢使用有使用限制的谷歌產品？

我們也不應期望能迎頭趕上。現代互聯網之所以依賴開源，是有原因的。開源具有我們無法模仿的一些顯著優勢。

## 我們需要他們比他們需要我們更多

始終保持我們技術的秘密本來就是一個薄弱的立場。谷歌的研究員們以固定節奏離職去其他公司，所以我們可以假定他們知道我們所知道的一切，並且只要這條通道保持開放，他們就會繼續這樣做。

但在LLM（大型語言模型）的前沿研究變得負擔得起的情況下，保持技術競爭優勢變得更加困難。世界各地的研究機構在彼此的基礎上建立成果，以一種遠超我們自身能力的廣度優先的方式探索解決方案空間。我們可以努力緊緊抱住我們的秘密，而外部創新削弱它們的價值，或者我們可以努力互相學習。

## 與公司相比，個人受許可限制的程度要低得多

這些創新大部分是基於Meta洩露的模型權重之上。雖然隨著真正的開源模型變得更好，這種情況必然會改變，但重點是他們不必等待。法律對“個人使用”的保護和起訴個人的不切實際性意味著個人在這些技術火熱的時候就可以獲得這些技術。

## 作為自己的客戶意味著您了解用例
瀏覽人們在圖像生成領域創建的模型，有著大量的創意涌現，從動漫生成器到HDR風景。這些模型是由深度沉浸在特定子類型的人們使用和創建的，這使得他們具有深厚的知識和同理心，這是我們無法望其項背的。

與其競爭，我們應該與開源領域合作。我們應該重視與第三方整合，並充分利用開放資源。這不僅有助於我們提高自己的產品和服務，還有助於擴大市場。

人們不會為具有使用限制的模型付費，而免費、無限制的替代品在質量上具有可比性。我們應該思考我們的價值增值究竟在哪裡。

大型模型讓我們速度變慢。從長遠來看，最好的模型是那些可以快速迭代的模型。現在我們知道在<20B參數範疇內有可能實現什麼，我們應該讓小變種更多地成為我們的思考對象。

擁有生態系統：讓開源為我們效力
矛盾的是，所有這些中唯一明確的贏家是Meta。因為洩露的模型是他們的，所以他們實際上獲得了整個地球的免費勞動力。由於大部分開源創新都是基於他們的架構之上，因此他們可以直接將其納入自己的產品。

擁有生態系統的價值不容忽視。Google本身已經成功地在其開源產品中使用了這一範例，例如Chrome和Android。通過擁有創新發生的平台，Google將自己鞏固為思想領袖和方向制定者，獲得塑造比自身更大思想敘事的能力。

我們對模型的控制越緊密，就越能使開放替代品變得更有吸引力。Google和OpenAI都在防守上趨向於發布模式，這使他們能夠緊密控制模型的使用方式。但這種控制是虛構的。任何想要將LLM用於未經批准的目的的人都可以選擇免費提供的模型。

Google應該在開源社區中建立自己的領導地位，與更廣泛的對話合作，而不是忽視它。這可能意味著採取一些令人不安的措施，例如發布小型ULM變體的模型權重。這必然意味著放棄對我們模型的部分控制。但這種妥協是不可避免的。我們無法在推動創新和控制創新之間兼顧。

## 後記：OpenAI呢？
所有關於開源的談話可能讓人覺得不公平，因為OpenAI目前的封閉政策。如果他們不分享，為什麼我們要分享？但事實上，我們已經在以挖走高級研究員的形式與他們分享一切。除非我們遏制這股潮流，否則保密是毫無意義的。

最後，OpenAI並不重要。他們在與開源相對立的立場上犯了與我們一樣的錯誤，因此他們保持競爭優勢的能力必然受到質疑。開源替代品可以並最終會取代他們，除非他們改變立場。至少在這一方面，我們可以先行一步。

## 時間表

| 日期         | 事件                                         | 描述                                                                                                                                                                                                                                                                                                                                                                       |
|------------|------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 2023年2月24日 | LLaMA 啟動                                 | Meta 推出 LLaMA，開放原始碼，但未公開權重。此時，LLaMA 沒有指令或對話調校。與許多當前模型一樣，它是一個相對較小的模型（可選擇的參數有 7B、13B、33B 和 65B），經過相對較長時間的訓練，因此相對於其大小具有相當的能力。                                                                                                                                     |
| 2023年3月3日  | 無法避免的事情發生                          | 一周內，LLaMA 被泄露給公眾。對社區的影響不可估量。現有許可證禁止將其用於商業目的，但突然之間，任何人都可以進行實驗。從這時起，創新接踵而至。                                                                                                                                                                                                                             |
| 2023年3月12日 | 吐司機上的語言模型                          | 一周多以後，Artem Andreenko 讓模型在樹莓派上運行。此時，由於權重必須在內存中進行分頁，模型運行速度過慢，無法實用。然而，這為後續的迷你化努力鋪平了道路。                                                                                                                                                                                                                |
| 2023年3月13日 | 筆記本電腦上的微調                          | 第二天，斯坦福大學發布了 Alpaca，為 LLaMA 添加了指令調校。然而，比實際權重更重要的是 Eric Wang 的 alpaca-lora 倉庫，該倉庫使用低秩微調在“單個 RTX 4090 上花費幾個小時”完成此訓練。                                                                                                                                                             |
| 2023年3月18日 | 現在它很快了                                 | Georgi Gerganov 使用 4 位量化使 LLaMA 在 MacBook CPU 上運行。這是第一個足夠快的“無 GPU”解決方案，可以實際使用。                                                                                                                                                                                                                              |
| 2023年3月19日 | 13B模型與Bard達到“對等”                   | 第二天，一個跨大學合作團隊發布了 Vicuna，並使用 GPT-4 驅動的 eval 對模型輸出進行定性比較。雖然評估              |
| 2023年3月25日 | 選擇您自己的模型                         | Nomic 創建了 GPT4All，既是一個模型，更重要的是一個生態系統。我們首次看到模型（包括 Vicuna）在一個地方聚集在一起。訓練成本：100美元。         |
| 2023年3月28日 | 開源 GPT-3                             | Cerebras（不要與我們自己的 Cerebra 混淆）使用 Chinchilla 隱含的最佳計算計劃和 μ-參數化的最佳擴展訓練 GPT-3 架構。這在很大程度上超越了現有的 GPT-3 克隆，並代表了“現實中”的第一個μ-參數化的確認使用。這些模型是從頭開始訓練的，這意味著社區不再依賴於 LLaMA。                                                                                                                     |
| 2023年3月28日 | 一小時內的多模態訓練                   | 使用一種新穎的參數高效微調（PEFT）技術，LLaMA-Adapter 在一小時內引入了指令調校和多模態性。令人印象深刻的是，他們只使用了120萬可學習參數。該模型在多模態 ScienceQA 上實現了新的 SOTA。                                                                                                                                                                                                                   |
| 2023年4月3日  | 真正的人類無法區分 13B開源模型和 ChatGPT | 伯克利大學推出了 Koala，這是一個完全使用可免費獲得的數據進行訓練的對話模型。他們採取了衡量真實人類在他們的模型和 ChatGPT 之間的偏好的關鍵一步。儘管 ChatGPT 仍然具有微弱的優勢，但超過 50% 的時間，用戶要么偏愛 Koala，要么沒有偏好。訓練成本：100美元。                                                                                                            |
| 2023年4月15日 | ChatGPT 等級的開源 RLHF                | Open Assistant 推出了一個模型，更重要的是，推出了一個通過 RLHF 進行對齊的數據集。在人類偏好方面，他們的模型接近 ChatGPT（48.3% vs. 51.7%）。除了 LLaMA 之外，他們還展示了該數據集可以應用於 Pythia-12B，讓人們有機會使用完全開放的技術棧來運行該模型。此外，由於數據集是公開可用的，它使得 RLHF 對於小型實驗者而言從無法實現變得便宜並且容易。
 |
 

2023年2月24日 - LLaMA 啟動
Meta 推出 LLaMA，開放原始碼，但未公開權重。此時，LLaMA 沒有指令或對話調校。與許多當前模型一樣，它是一個相對較小的模型（可選擇的參數有 7B、13B、33B 和 65B），經過相對較長時間的訓練，因此相對於其大小具有相當的能力。

2023年3月3日 - 無法避免的事情發生
一周內，LLaMA 被泄露給公眾。對社區的影響不可估量。現有許可證禁止將其用於商業目的，但突然之間，任何人都可以進行實驗。從這時起，創新接踵而至。

2023年3月12日 - 吐司機上的語言模型
一周多以後，Artem Andreenko 讓模型在樹莓派上運行。此時，由於權重必須在內存中進行分頁，模型運行速度過慢，無法實用。然而，這為後續的迷你化努力鋪平了道路。

2023年3月13日 - 筆記本電腦上的微調
第二天，斯坦福大學發布了 Alpaca，為 LLaMA 添加了指令調校。然而，比實際權重更重要的是 Eric Wang 的 alpaca-lora 倉庫，該倉庫使用低秩微調在“單個 RTX 4090 上花費幾個小時”完成此訓練。

突然間，任何人都可以微調模型以做任何事情，引發了一場以低預算微調項目為底的競賽。論文自豪地描述了他們總共花費了幾百美元。更重要的是，這些低秩更新可以輕鬆地與原始權重分開分發，使它們獨立於 Meta 的原始許可證。任何人都可以分享和應用它們。

2023年3月18日 - 現在它很快了
Georgi Gerganov 使用 4 位量化使 LLaMA 在 MacBook CPU 上運行。這是第一個足夠快的“無 GPU”解決方案，可以實際使用。

2023年3月19日 - 13B模型與Bard達到“對等”
第二天，一個跨大學合作團隊發布了 Vicuna，並使用 GPT-4 驅動的 eval 對模型輸出進行定性比較。雖然評估方法存在疑問，但該模型在材料上比早期版本更好。訓練成本：300美元。

值得注意的是，他們能夠使用 ChatGPT 的數據，同時繞過其 API 的限制 - 他們只需選擇在像 ShareGPT 這樣的網站上發布的“令人印象深刻”的 ChatGPT 對話示例。

2023年3月25日 - 選擇您自己的模型
Nomic 創建了 GPT4All，既是一個模型，更重要的是一個生態系統。我們首次看到模型（包括 Vicuna）在一個地方聚集在一起。訓練成本：100美元。

2023年3月28日 - 開源 GPT-3
Cerebras（不要與我們自己的 Cerebra 混淆）使用 Chinchilla 隱含的最佳計算計劃和 μ-參數化的最佳擴展訓練 GPT-3 架構。這在很大程度上超越了現有的 GPT-3 克隆，並代表了“現實中”的第一個μ-參數化的確認使用。這些模型是從頭開始訓練的，這意味著社區不再依賴於 LLaMA。

2023年3月28日 - 一小時內的多模態訓練
使用一種新穎的參數高效微調（PEFT）技術，LLaMA-Adapter 在一小時內引入了指令調校和多模態性。令人印象深刻的是，他們只使用了120萬可學習參數。該模型在多模態 ScienceQA 上實現了新的 SOTA。

2023年4月3日 - 真正的人類無法區分 13B開源模型和 ChatGPT
伯克利大學推出了 Koala，這是一個完全使用可免費獲得的數據進行訓練的對話模型。

他們採取了衡量真實人類在他們的模型和 ChatGPT 之間的偏好的關鍵一步。

儘管 ChatGPT 仍然具有微弱的優勢，但超過 50% 的時間，用戶要么偏愛 Koala，要么沒有偏好。訓練成本：100美元。

2023年4月15日 - ChatGPT 等級的開源 RLHF
Open Assistant 推出了一個模型，更重要的是，推出了一個通過 RLHF 進行對齊的數據集。在人類偏好方面，他們的模型接近 ChatGPT（48.3% vs. 51.7%）。除了 LLaMA 之外，他們還展示了該數據集可以應用於 Pythia-12B，讓人們有機會使用完全開放的技術棧來運行該模型。此外，由於數據集是公開可用的，它使得 RLHF 對於小型實驗者而言從無法實現變得便宜並且容易。
