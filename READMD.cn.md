以下文字是一份最近被泄露的文件，由一位匿名人士在公共Discord服务器上分享，并已授权重新发布。它来自谷歌内部的一名研究员。我们已经核实了其真实性。唯一的修改是格式调整和删除了指向内部网页的链接。该文件仅代表谷歌员工的观点，而非整个公司。我们并不同意下面所写的观点，我们咨询的其他研究人员也不同意，但我们将为订阅者在另一篇文章中发表我们的意见。我们只是用来分享这个引发了很多有趣观点的文件的工具。

## 我们没有护城河

### OpenAI也没有

我们一直在关注OpenAI的动态。谁会达到下一个里程碑？下一步会是什么？

但令人不安的事实是，我们并没有摆好位置赢得这场军备竞赛，OpenAI也是如此。在我们争论的时候，第三方势力已经在悄悄地抢占我们的市场份额。

我当然是在谈论开源。简单地说，他们已经把我们甩在身后。我们认为的“重大开放性问题”已经得到解决，并被广泛应用。仅举几个例子：

 - 手机上的LLMs：人们可以在Pixel 6手机上以每秒5个令牌的速度运行基础模型。
 - 可扩展的个人AI：你可以在一个晚上在你的笔记本电脑上微调一个个性化的AI。
 - 负责任的发布：这个问题并没有被“解决”，而是被“消除”。有整个网站充满了没有任何限制的艺术模型，而文本也即将迎头赶上。
 - 多模态：目前多模态ScienceQA领域的最新技术成果仅用了一个小时就训练出来了。

虽然我们的模型在质量上仍然占有轻微优势，但差距正在以惊人的速度缩小。开源模型更快、更具可定制性、更注重隐私，性价比更高。他们用100美元和130亿参数做到了我们在1000万美元和5400亿参数下努力实现的事情。而且他们只花了几周，而不是几个月。这对我们有深远的影响：

 - 我們沒有秘密武器。我們最好的希望是學習和與谷歌以外的其他人合作。我們應該優先考慮支持第三方整合。
 - 當免費、無限制的替代品在質量上相當時，人們不會為受限制的模型付費。我們應該考慮我們的附加價值究竟在哪裡。
 - 巨大的模型正在拖慢我們的步伐。從長遠來看，最好的模型是那些可以快速迭代的模型。現在我們知道在<20B參數範疇內有什麼可能，我們應該將小型變體視為不只是一個附帶的想法。

## 發生了什麼事

三月初，開源社區得到了他們第一個真正有能力的基礎模型，因為Meta的LLaMA被泄露給了公眾。它沒有指令或對話調整，也沒有RLHF。然而，社區立刻意識到了他們得到的東西的重要性。

隨後是一波巨大的創新浪潮，每隔幾天就有重大的發展（詳見完整的時間表）。短短一個月後，我們看到了帶有指令調整、量化、質量改進、人類評估、多模態、RLHF等等功能的變體，其中很多是相互構建的。

最重要的是，他們在很大程度上解決了擴展問題，使得任何人都可以嘗試。許多新想法來自普通人。訓練和實驗的門檻已經從一個大型研究機構的全部產出降低到一個人、一個晚上和一台性能強大的筆記本電腦。

## 為什麼我們本可以預見

在很多方面，這對任何人來說都不應該是一個驚喜。開源LLM的當前復興緊跟在圖像生成復興之後。社區並未忽略這些相似之處，許多人將這稱為LLM的“穩定擴散時刻”。

在這兩種情況下，通過稱為低秩適應（LoRA）的更便宜的微調機制，以及規模上的重大突破（用於圖像合成的潛在擴散，用於LLM的Chinchilla），都使得低成本的公共參與成為可能。在這兩種情況下，獲得足夠高質量的模型引發了來自世界各地個人和機構的一系列想法和迭代。在這兩種情況下，這些迅速超越了大型參與者。

在圖像生成領域，這些貢獻起到了關鍵作用，使穩定擴散與Dall-E走上了不同的道路。擁有開放模型導致了產品整合、市場、用戶界面和創新，這些都是Dall-E未曾實現的。

效果是顯而易見的：在文化影響方面迅速壓倒了OpenAI的解決方案，後者變得越來越無關緊要。對於LLM是否會發生同樣的事情，尚有待觀察，但結構層面的相似性是相同的。

## 我們錯過了什麼
驅動開源近期成功的創新直接解決了我們仍在努力解決的問題。更加關注他們的工作可以幫助我們避免重複發明輪子。

LoRA是一個非常強大的技術，我們可能應該更加關注
LoRA通過將模型更新表示為低秩分解來工作，這可以將更新矩陣的大小降低多達幾千倍。這使得模型微調的成本和時間大大降低。在幾個小時內在消費者硬件上個性化語言模型是一個重要的突破，特別是對於涉及在近實時內納入新的和多樣化知識的願景。這項技術的存在在谷歌內部被低估了，儘管它直接影響到我們一些最具野心的項目。

## 重新訓練模型是困難的道路

使LoRA如此有效的部分原因是它（像其他形式的微調一樣）是可堆疊的。像指令調整這樣的改進可以應用，然後在其他貢獻者添加對話、推理或工具使用時獲得優勢。雖然單個微調是低秩的，但它們的總和不一定是，允許隨著時間的推移，對模型進行全秩更新。

這意味著隨著新的更好的數據集和任務變得可用，模型可以以低成本保持最新狀態，而無需支付完整運行的成本。

相比之下，從頭開始訓練巨型模型不僅丟掉了預訓練，還丟掉了已經在頂部進行的迭代改進。在開源世界，這些改進很快就會佔據主導地位，使得完全重新訓練變得極為昂貴。

我們應該仔細考慮每個新的應用或想法是否真的需要一個全新的模型。如果我們確實有重大的架構改進，使我們無法直接重用模型權重，那麼我們應該投資於更具侵略性的蒸餾形式，以保留上一代模型盡可能多的功能。

